{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preprocessing and creating a model in a Machine Learning workflow, it is fundamental to ensure that every dataset is processed in the same way every time. To this extent, pipelines play a key role ensuring the same process for every dataset we want to import and use.\n",
    "\n",
    "A pipeline can be defined as a sequence of objects that act on a set of data. The actions can include:\n",
    "\n",
    "- apply transformations\n",
    "- impute missing values (mean, median, zero, etc.)\n",
    "- create a new feature\n",
    "- fit a model\n",
    "- predict on unseen data\n",
    "\n",
    "The purpose of this is to validate the ML process. Indeed, whenever a part of the process changes, it is really useful to understand whether that change improved the performance or not. This could be the creation of a new feature or imputing missing values with the mean value of that feature rather than with zero. \n",
    "\n",
    "A trustable evaluation of the process is needed to get all this insights, else you will not be able to judge whether a specific change is improving or worsening your model performances. The definiton of a pipeline is fundamental to this extent, as repeating all those actions manually could easily lead to errors along the ML workflow.\n",
    "\n",
    "First, let's import all the needed libraries and the dataset to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df_train_filepath = r'.\\dataset\\train.csv'\n",
    "df_train = pd.read_csv(df_train_filepath)\n",
    "\n",
    "df_test_filepath = r'.\\dataset\\test.csv'\n",
    "df_test = pd.read_csv(df_test_filepath)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "def make_test(train, test_size, random_state, strat_feat=None):\n",
    "    if strat_feat:\n",
    "        \n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        for train_index, test_index in split.split(train, train[strat_feat]):\n",
    "            train_set = train.loc[train_index]\n",
    "            test_set = train.loc[test_index]\n",
    "            \n",
    "    return train_set, test_set\n",
    "\n",
    "# Split the dataset while maintaining the proportion of 'Neighborhood'\n",
    "\n",
    "train_set, test_set = make_test(df_train, \n",
    "                                test_size=0.2, random_state=654, \n",
    "                                strat_feat='Neighborhood')\n",
    "\n",
    "print(\"\\nTraining Set:\\n\", train_set)\n",
    "print(\"\\nTest Set:\\n\", test_set)\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the classes that are simple wrappers of already existing classes. We do this because using for example the SimpleImputer class, the output would be a numpy object while we want to keep the DataFrame object along our ML workflow. To this extent, customized classes can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class general_cleaner(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This class applies what we know from the documetation.\n",
    "    It cleans some known missing values\n",
    "    If flags the missing values\n",
    "\n",
    "    This process is supposed to happen as first step of any pipeline\n",
    "    '''\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        #LotFrontage\n",
    "        X.loc[X.LotFrontage.isnull(), 'LotFrontage'] = 0\n",
    "        #Alley\n",
    "        X.loc[X.Alley.isnull(), 'Alley'] = \"NoAlley\"\n",
    "        #MSSubClass\n",
    "        X['MSSubClass'] = X['MSSubClass'].astype(str)\n",
    "        #MissingBasement\n",
    "        fil = ((X.BsmtQual.isnull()) & (X.BsmtCond.isnull()) & (X.BsmtExposure.isnull()) &\n",
    "              (X.BsmtFinType1.isnull()) & (X.BsmtFinType2.isnull()))\n",
    "        fil1 = ((X.BsmtQual.notnull()) | (X.BsmtCond.notnull()) | (X.BsmtExposure.notnull()) |\n",
    "              (X.BsmtFinType1.notnull()) | (X.BsmtFinType2.notnull()))\n",
    "        X.loc[fil1, 'MisBsm'] = 0\n",
    "        X.loc[fil, 'MisBsm'] = 1 # made explicit for safety\n",
    "        #BsmtQual\n",
    "        X.loc[fil, 'BsmtQual'] = \"NoBsmt\" #missing basement\n",
    "        #BsmtCond\n",
    "        X.loc[fil, 'BsmtCond'] = \"NoBsmt\" #missing basement\n",
    "        #BsmtExposure\n",
    "        X.loc[fil, 'BsmtExposure'] = \"NoBsmt\" #missing basement\n",
    "        #BsmtFinType1\n",
    "        X.loc[fil, 'BsmtFinType1'] = \"NoBsmt\" #missing basement\n",
    "        #BsmtFinType2\n",
    "        X.loc[fil, 'BsmtFinType2'] = \"NoBsmt\" #missing basement\n",
    "        #BsmtFinSF1\n",
    "        X.loc[fil, 'BsmtFinSF1'] = 0 # No bsmt\n",
    "        #BsmtFinSF2\n",
    "        X.loc[fil, 'BsmtFinSF2'] = 0 # No bsmt\n",
    "        #BsmtUnfSF\n",
    "        X.loc[fil, 'BsmtUnfSF'] = 0 # No bsmt\n",
    "        #TotalBsmtSF\n",
    "        X.loc[fil, 'TotalBsmtSF'] = 0 # No bsmt\n",
    "        #BsmtFullBath\n",
    "        X.loc[fil, 'BsmtFullBath'] = 0 # No bsmt\n",
    "        #BsmtHalfBath\n",
    "        X.loc[fil, 'BsmtHalfBath'] = 0 # No bsmt\n",
    "        #FireplaceQu\n",
    "        X.loc[(X.Fireplaces == 0) & (X.FireplaceQu.isnull()), 'FireplaceQu'] = \"NoFire\" #missing\n",
    "        #MisGarage\n",
    "        fil = ((X.GarageYrBlt.isnull()) & (X.GarageType.isnull()) & (X.GarageFinish.isnull()) &\n",
    "              (X.GarageQual.isnull()) & (X.GarageCond.isnull()))\n",
    "        fil1 = ((X.GarageYrBlt.notnull()) | (X.GarageType.notnull()) | (X.GarageFinish.notnull()) |\n",
    "              (X.GarageQual.notnull()) | (X.GarageCond.notnull()))\n",
    "        X.loc[fil1, 'MisGarage'] = 0\n",
    "        X.loc[fil, 'MisGarage'] = 1\n",
    "        #GarageYrBlt\n",
    "        X.loc[X.GarageYrBlt > 2200, 'GarageYrBlt'] = 2007 #correct mistake\n",
    "        X.loc[fil, 'GarageYrBlt'] = 0\n",
    "        #GarageType\n",
    "        X.loc[fil, 'GarageType'] = \"NoGrg\" #missing garage\n",
    "        #GarageFinish\n",
    "        X.loc[fil, 'GarageFinish'] = \"NoGrg\" #missing\n",
    "        #GarageQual\n",
    "        X.loc[fil, 'GarageQual'] = \"NoGrg\" #missing\n",
    "        #GarageCond\n",
    "        X.loc[fil, 'GarageCond'] = \"NoGrg\" #missing\n",
    "        #Fence\n",
    "        X.loc[X.Fence.isnull(), 'Fence'] = \"NoFence\" #missing fence\n",
    "        #Pool\n",
    "        fil = ((X.PoolArea == 0) & (X.PoolQC.isnull()))\n",
    "        X.loc[fil, 'PoolQC'] = 'NoPool' \n",
    "        \n",
    "        del X['Id']\n",
    "        del X['MiscFeature']\n",
    "        del X['MSSubClass']\n",
    "        del X['Neighborhood']  # this should be useful\n",
    "        del X['Condition1']\n",
    "        del X['Condition2']\n",
    "        del X['ExterCond']  # maybe ordinal\n",
    "        del X['Exterior1st']\n",
    "        del X['Exterior2nd']\n",
    "        del X['Functional']\n",
    "        del X['Heating']\n",
    "        del X['PoolQC']\n",
    "        del X['RoofMatl']\n",
    "        del X['RoofStyle']\n",
    "        del X['SaleCondition']\n",
    "        del X['SaleType']\n",
    "        del X['Utilities']\n",
    "        del X['BsmtCond']\n",
    "        del X['Electrical']\n",
    "        del X['Foundation']\n",
    "        del X['Street']\n",
    "        del X['Fence']\n",
    "        del X['LandSlope']\n",
    "        \n",
    "        return X\n",
    "\n",
    "class df_imputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Just a wrapper for the SimpleImputer that keeps the dataframe structure\n",
    "    '''\n",
    "    def __init__(self, strategy='mean'):\n",
    "        self.strategy = strategy\n",
    "        self.imp = None\n",
    "        self.statistics_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imp = SimpleImputer(strategy=self.strategy)\n",
    "        self.imp.fit(X)\n",
    "        self.statistics_ = pd.Series(self.imp.statistics_, index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X is supposed to be a DataFrame\n",
    "        Ximp = self.imp.transform(X)\n",
    "        Xfilled = pd.DataFrame(Ximp, index=X.index, columns=X.columns)\n",
    "        return Xfilled\n",
    "    \n",
    "    \n",
    "class df_scaler(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Wrapper of StandardScaler or RobustScaler\n",
    "    '''\n",
    "    def __init__(self, method='standard'):\n",
    "        self.scl = None\n",
    "        self.scale_ = None\n",
    "        self.method = method\n",
    "        if self.method == 'sdandard':\n",
    "            self.mean_ = None\n",
    "        elif method == 'robust':\n",
    "            self.center_ = None\n",
    "        self.columns = None  # this is useful when it is the last step of a pipeline before the model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'standard':\n",
    "            self.scl = StandardScaler()\n",
    "            self.scl.fit(X) # fit will learn the parameters needed to scale the data\n",
    "            self.mean_ = pd.Series(self.scl.mean_, index=X.columns)\n",
    "        elif self.method == 'robust':\n",
    "            self.scl = RobustScaler()\n",
    "            self.scl.fit(X) # fit will learn the parameters needed to scale the data\n",
    "            self.center_ = pd.Series(self.scl.center_, index=X.columns)\n",
    "        self.scale_ = pd.Series(self.scl.scale_, index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame, in such a way we applied the transform (scaling) while keeping the DataFrame\n",
    "        #print(f\"The previous saved columns during fit were: {self.scl.feature_names_in_}\")\n",
    "        #print(f\"Now the columns used are: {X.columns}\")\n",
    "        # Compare the columns of the DataFrame with feature_names_in_\n",
    "        #are_columns_equal = X.columns.equals(pd.Index(self.scl.feature_names_in_))\n",
    "        #print(\"Columns match with feature_names_in_:\", are_columns_equal)\n",
    "        X = X.loc[:, self.scl.feature_names_in_]\n",
    "        Xscl = self.scl.transform(X)\n",
    "        \n",
    "        #print(self.scl.feature_names_in_)\n",
    "        Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n",
    "        self.columns = X.columns\n",
    "        return Xscaled\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return list(self.columns)  # this is going to be useful when coupled with FeatureUnion\n",
    "    \n",
    "\n",
    "class dummify(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Wrapper for get dummies, this is used to turn the categorical features into one-hot encodes.\n",
    "    The different values of that feature will become new features. No need to fit anything. \n",
    "    '''\n",
    "    def __init__(self, drop_first=False, match_cols=True):\n",
    "        self.drop_first = drop_first\n",
    "        self.columns = []  # useful to well behave with FeatureUnion\n",
    "        self.match_cols = match_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = []  # for safety, when we refit we want new columns\n",
    "        return self\n",
    "    \n",
    "    def match_columns(self, X):\n",
    "        miss_train = list(set(X.columns) - set(self.columns))\n",
    "        miss_test = list(set(self.columns) - set(X.columns))\n",
    "        \n",
    "        err = 0\n",
    "        \n",
    "        if len(miss_test) > 0:\n",
    "            for col in miss_test:\n",
    "                X[col] = 0  # insert a column for the missing dummy\n",
    "                err += 1\n",
    "        if len(miss_train) > 0:\n",
    "            for col in miss_train:\n",
    "                del X[col]  # delete the column of the extra dummy\n",
    "                err += 1\n",
    "                \n",
    "        if err > 0:\n",
    "            warnings.warn('The dummies in this set do not match the ones in the train set, we corrected the issue.',\n",
    "                         UserWarning) # this is to avoid that in the test dataset there are less dummies, this will take care of them assigning 0 to them, all this to not break the pipeline\n",
    "        return X\n",
    "    \n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = pd.get_dummies(X, drop_first=self.drop_first)\n",
    "        if (len(self.columns) > 0): \n",
    "            if self.match_cols:\n",
    "                X = self.match_columns(X)\n",
    "            self.columns = X.columns\n",
    "        else:\n",
    "            self.columns = X.columns\n",
    "        return X\n",
    "    \n",
    "    def get_features_name(self):\n",
    "        return self.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummify class in machine learning is useful for encoding categorical features using one-hot encoding. Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_set[['HouseStyle']].copy()\n",
    "dummifier = dummify()\n",
    "tmp = dummifier.transform(tmp)\n",
    "tmp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, before the categorical variable HouseStyle had different values (1.5Fin, 1.5Unf, 1Story, etc.) and they have been one-hot encoded and turned into dummy variables.\n",
    "\n",
    "Anyways, in every ML workflow mostly of the times the first transformer to be applied will likely be the imputer. The imputation method will change if dealing with numerical variables or categorical ones. Thus, the following class is to first split numerical and categorical variables into two subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feat_sel(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This transformer selects either numerical or categorical features.\n",
    "    In this way we can build separate pipelines for separate data types.\n",
    "    '''\n",
    "    def __init__(self, dtype='numeric'):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit( self, X, y=None ):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.dtype == 'numeric':\n",
    "            num_cols = X.columns[X.dtypes != object].tolist()\n",
    "            return X[num_cols]\n",
    "        elif self.dtype == 'category':\n",
    "            cat_cols = X.columns[X.dtypes == object].tolist()\n",
    "            return X[cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in order to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_set.copy()\n",
    "selector = feat_sel()  # it is numeric by default\n",
    "tmp = selector.transform(tmp)  # no reason to fit again\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when working with categorical features some categories may be really rare, implying a mismatch in the dimension between test and train datasets. To take care of this issue, our dummify class will add the columns to not have any dimensional mismatch between train and test datasets. Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_set[['RoofMatl']].copy()\n",
    "dummifier = dummify()\n",
    "dummifier.fit_transform(tmp).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but plotting out the test dataset, we can see that we do not have all those categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.RoofMatl.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and dummifying this categorical variable would lead to creating just 4 columns, one for each category present. Though, the custom dummifier takes care of that in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = test_set[['RoofMatl']].copy()\n",
    "dummifier.transform(tmp).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create our customized pipeline to manage numerical features. Ideally, we want it to be as follows:\n",
    "- cleaning the data\n",
    "- impute some missing values with mean, median, etc.\n",
    "- apply some transformation on some features\n",
    "- create new features\n",
    "- scale the data\n",
    "\n",
    "First let's create a transformer for the numeric variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tr_numeric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, SF_room=True):\n",
    "        self.columns = []  # useful to well behave with FeatureUnion\n",
    "        self.SF_room = SF_room\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def remove_skew(self, X, column):\n",
    "        X[column] = np.log1p(X[column])\n",
    "        return X\n",
    "\n",
    "\n",
    "    def SF_per_room(self, X):\n",
    "        if self.SF_room:\n",
    "            X['sf_per_room'] = X['GrLivArea'] / X['TotRmsAbvGrd']\n",
    "        return X\n",
    "    \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for col in ['GrLivArea', '1stFlrSF', 'LotArea']: # they can also be inputs\n",
    "            X = self.remove_skew(X, col)\n",
    "\n",
    "        X = self.SF_per_room(X)\n",
    "        \n",
    "        self.columns = X.columns \n",
    "        return X\n",
    "    \n",
    "\n",
    "    def get_features_name(self):  # again, it will be useful later\n",
    "        return self.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this transformer takes a parameter that determines whether or not to create a new feature. It is this kind of parameter that can be tuned with a GridSearch (more on this later).\n",
    "\n",
    "Creating a new feature in that way would be impossible if the previous steps were not returning a DataFrame. There is naturally an alternative that includes specifying the index of the columns you want to use, but I find this approach way more user-friendly and robust.\n",
    "\n",
    "A pipeline for numeric features would then look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # select only the numeric features\n",
    "                         ('imputer', df_imputer(strategy='median')),  # impute the missing values with the median of each column\n",
    "                         ('transf', tr_numeric(SF_room=True)),  # remove skew and create a new feature\n",
    "                         ('scl', df_scaler(method='standard'))])  # scale the data\n",
    "\n",
    "full_pipe = Pipeline([('gen_cl', general_cleaner()), ('num_pipe', numeric_pipe)])  # put the cleaner on top because we like it clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, with the use of the sklearn Pipeline, we want to sequentially apply the transformations in the given list. The list is made of tuples, the first element is a label for that step, and the second element is the transformation (or the model, or another pipeline). The name is useful to identify every parameter of the Pipeline, as we will see later.\n",
    "\n",
    "This pipeline, given the training data, acts as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_set.copy()\n",
    "tmp = full_pipe.fit_transform(tmp)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we wanted, the data flew through the pipeline, getting cleaned, transformed, and rescaled. Moreover, we still have a nice DataFrame structure.\n",
    "\n",
    "The powerfulness of this pipeline is visible when we want to do the same thing to the validation set and it is evident when we implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = test_set.copy()  # not ready to work on those sets yet\n",
    "tmp = full_pipe.transform(tmp)  # the fit already happened with the training set, we don't want to fit again\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the pipeline the fit method is used by the scaler to get the parameters (mean value if StandardScaler, median value if RobustScaler) while the transform method is used to apply the transformation to the dataset. In all the ML applications when scaling the dataset to be used for training, it is important to keep in mind that the parameters have to be computed only on the training dataset and not including the testing dataset. This is done because computing the the mean value on the whole dataset would add an information about the testing data inside the training one, that is not what we want. We want to keep the testing data unseen and use it for inference! This is why when applying the pipeline to the testing dataset we do not want to fit again.\n",
    "\n",
    "Though, the full pipeline will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead for categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_ordinal(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Transforms ordinal features in order to have them as numeric (preserving the order)\n",
    "    If unsure about converting or not a feature (maybe making dummies is better), make use of\n",
    "    extra_cols and include_extra\n",
    "    '''\n",
    "    def __init__(self, cols, extra_cols=None, include_extra=True):\n",
    "        self.cols = cols\n",
    "        self.extra_cols = extra_cols\n",
    "        self.mapping = {'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n",
    "        self.include_extra = include_extra\n",
    "    \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.extra_cols:\n",
    "            if self.include_extra:\n",
    "                self.cols += self.extra_cols\n",
    "            else:\n",
    "                for col in self.extra_cols:\n",
    "                    del X[col]\n",
    "        \n",
    "        for col in self.cols:\n",
    "            X.loc[:, col] = X[col].map(self.mapping).fillna(0)\n",
    "        return X\n",
    "\n",
    "    \n",
    "class recode_cat(BaseEstimator, TransformerMixin):        \n",
    "    '''\n",
    "    Recodes some categorical variables according to the insights gained from the\n",
    "    data exploration phase. Not presented in this notebook\n",
    "    '''\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def tr_GrgType(self, data):\n",
    "        data['GarageType'] = data['GarageType'].map({'Basment': 'Attchd',\n",
    "                                                  'CarPort': 'Detchd', \n",
    "                                                  '2Types': 'Attchd' }).fillna(data['GarageType'])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_LotShape(self, data):\n",
    "        fil = (data.LotShape != 'Reg')\n",
    "        data['LotShape'] = 1\n",
    "        data.loc[fil, 'LotShape'] = 0\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_LandCont(self, data):\n",
    "        fil = (data.LandContour == 'HLS') | (data.LandContour == 'Low')\n",
    "        data['LandContour'] = 0\n",
    "        data.loc[fil, 'LandContour'] = 1\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_LandSlope(self, data):\n",
    "        fil = (data.LandSlope != 'Gtl')\n",
    "        data['LandSlope'] = 0\n",
    "        data.loc[fil, 'LandSlope'] = 1\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_MSZoning(self, data):\n",
    "        data['MSZoning'] = data['MSZoning'].map({'RH': 'RM', # medium and high density\n",
    "                                                 'C (all)': 'RM', # commercial and medium density\n",
    "                                                 'FV': 'RM'}).fillna(data['MSZoning'])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_Alley(self, data):\n",
    "        fil = (data.Alley != 'NoAlley')\n",
    "        data['Alley'] = 0\n",
    "        data.loc[fil, 'Alley'] = 1\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_LotConfig(self, data):\n",
    "        data['LotConfig'] = data['LotConfig'].map({'FR3': 'Corner', # corners have 2 or 3 free sides\n",
    "                                                   'FR2': 'Corner'}).fillna(data['LotConfig'])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_BldgType(self, data):\n",
    "        data['BldgType'] = data['BldgType'].map({'Twnhs' : 'TwnhsE',\n",
    "                                                 '2fmCon': 'Duplex'}).fillna(data['BldgType'])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def tr_MasVnrType(self, data):\n",
    "        data['MasVnrType'] = data['MasVnrType'].map({'BrkCmn': 'BrkFace'}).fillna(data['MasVnrType'])\n",
    "        return data\n",
    "\n",
    "\n",
    "    def tr_HouseStyle(self, data):\n",
    "        data['HouseStyle'] = data['HouseStyle'].map({'1.5Fin': '1.5Unf', \n",
    "                                                         '2.5Fin': '2Story', \n",
    "                                                         '2.5Unf': '2Story', \n",
    "                                                         'SLvl': 'SFoyer'}).fillna(data['HouseStyle'])\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = self.tr_GrgType(X)\n",
    "        X = self.tr_LotShape(X)\n",
    "        X = self.tr_LotConfig(X)\n",
    "        X = self.tr_MSZoning(X)\n",
    "        X = self.tr_Alley(X)\n",
    "        X = self.tr_LandCont(X)\n",
    "        X = self.tr_BldgType(X)\n",
    "        X = self.tr_MasVnrType(X)\n",
    "        X = self.tr_HouseStyle(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline for categorical features will then be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('fs', feat_sel(dtype='category')),\n",
    "                     ('imputer', df_imputer(strategy='most_frequent')), \n",
    "                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n",
    "                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n",
    "                     ('recode', recode_cat()), \n",
    "                     ('dummies', dummify())])\n",
    "\n",
    "full_pipe = Pipeline([('gen_cl', general_cleaner()), ('cat_pipe', cat_pipe)])\n",
    "\n",
    "tmp = train_set.copy()\n",
    "tmp = full_pipe.fit_transform(tmp)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, some categories converted into numeric features, other first recoded and the dummified. This dataset is ready for a model and, as before, this pipeline is ready for the validation set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = test_set.copy()\n",
    "tmp = full_pipe.transform(tmp)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to put everything together. We have a pipeline for numeric features, one for categorical one, now we want a complete pipeline for the entire dataset.\n",
    "\n",
    "Sklearn again helps us with FeatureUnion that, sadly, again compromises the DataFrame structure we are very much fun of. By now, we are confident enough to create our own version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureUnion_df(TransformerMixin, BaseEstimator):\n",
    "    '''\n",
    "    Wrapper of FeatureUnion but returning a Dataframe, \n",
    "    the column order follows the concatenation done by FeatureUnion\n",
    "\n",
    "    transformer_list: list of Pipelines\n",
    "\n",
    "    '''\n",
    "    def __init__(self, transformer_list, n_jobs=None, transformer_weights=None, verbose=False, **kwargs):\n",
    "        self.transformer_list = transformer_list\n",
    "        self.n_jobs = n_jobs\n",
    "        self.transformer_weights = transformer_weights\n",
    "        self.verbose = verbose  # these are necessary to work inside of GridSearch or similar\n",
    "        self.kwargs = kwargs  # Capture extra arguments\n",
    "        self.feat_un = FeatureUnion(self.transformer_list, \n",
    "                                    n_jobs=self.n_jobs, \n",
    "                                    transformer_weights=self.transformer_weights, \n",
    "                                    verbose=self.verbose,\n",
    "                                    **self.kwargs) # to fix additional key words passed by grid search to FeatureUnion\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.feat_un.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_tr = self.feat_un.transform(X)\n",
    "        columns = []\n",
    "        \n",
    "        for trsnf in self.transformer_list:\n",
    "            cols = trsnf[1].steps[-1][1].get_features_name()  # getting the features name from the last step of each pipeline\n",
    "            columns += list(cols)\n",
    "\n",
    "        X_tr = pd.DataFrame(X_tr, index=X.index, columns=columns)\n",
    "        \n",
    "        return X_tr\n",
    "\n",
    "    def get_params(self, deep=True):  # necessary to well behave in GridSearch\n",
    "        return self.feat_un.get_params(deep=deep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope it is now evident why I kept implementing a get_features_name method in the previous classes. It was all for this moment.\n",
    "\n",
    "The complete pipeline will then be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipe = Pipeline([('fs', feat_sel('numeric')),\n",
    "                         ('imputer', df_imputer(strategy='median')),\n",
    "                         ('transf', tr_numeric())])\n",
    "\n",
    "cat_pipe = Pipeline([('fs', feat_sel('category')),\n",
    "                     ('imputer', df_imputer(strategy='most_frequent')), \n",
    "                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n",
    "                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n",
    "                     ('recode', recode_cat()), \n",
    "                     ('dummies', dummify())])\n",
    "\n",
    "processing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', cat_pipe),\n",
    "                                                 ('num_pipe', numeric_pipe)], )\n",
    "\n",
    "full_pipe = Pipeline([('gen_cl', general_cleaner()), \n",
    "                      ('processing', processing_pipe), \n",
    "                      ('scaler', df_scaler())])  # the scaler is here to have also the ordinal features scaled\n",
    "\n",
    "tmp = df_train.copy()\n",
    "tmp = full_pipe.fit_transform(tmp)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the columns in our DataFrame object changed because FeatureUnion concatenated the results of each transformer.\n",
    "\n",
    "Again, we can now apply the pipeline to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = test_set.copy()\n",
    "tmp = full_pipe.transform(tmp)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the parameters are a bit more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having set up everything as we did, it is not difficult to tune our pipeline with GridSearch. We will put a simple model at the end of the pipeline just for the fun of it and tune both the hyperparameters of this model and the parameters of the pipeline.\n",
    "\n",
    "We thus make use of GridSearch to pick the best model configuration by varying several parameters, namely\n",
    "\n",
    "Whether or not we create the new feature describing the square feet per room\n",
    "If we impute the numerical missing values with the mean or the median\n",
    "If we drop one dummy or not\n",
    "If we change the regularization parameter of the Lasso regression\n",
    "Thanks to the fact that we have a pipeline, we are able to easily explore all these configurations without worrying too much about information leakage or by repeating the same steps over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "\n",
    "folds = KFold(5, shuffle=True, random_state=541)\n",
    "\n",
    "df_train['Target'] = np.log1p(df_train.SalePrice)\n",
    "\n",
    "del df_train['SalePrice']\n",
    "\n",
    "# Split the dataset while maintaining the proportion of 'Neighborhood'\n",
    "train_set, test_set = make_test(df_train, \n",
    "                                test_size=0.2, random_state=654, \n",
    "                                strat_feat='Neighborhood')\n",
    "\n",
    "y = train_set['Target'].copy()\n",
    "del train_set['Target']\n",
    "\n",
    "y_test = test_set['Target']\n",
    "del test_set['Target']\n",
    "\n",
    "\n",
    "def grid_search(data, target, estimator, param_grid, scoring, cv):\n",
    "    \n",
    "    grid = GridSearchCV(estimator=estimator, param_grid=param_grid, \n",
    "                        cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "    \n",
    "    pd.options.mode.chained_assignment = None  # this is because the gridsearch throws a lot of pointless warnings\n",
    "    tmp = data.copy()\n",
    "    grid = grid.fit(tmp, target)\n",
    "    pd.options.mode.chained_assignment = 'warn'\n",
    "    \n",
    "    result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_test_score', \n",
    "                                                        ascending=False).reset_index()\n",
    "    \n",
    "    del result['params']\n",
    "    times = [col for col in result.columns if col.endswith('_time')]\n",
    "    params = [col for col in result.columns if col.startswith('param_')]\n",
    "    \n",
    "    result = result[params + ['mean_test_score', 'std_test_score'] + times]\n",
    "    \n",
    "    return result, grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search (here in an utility function just to have better looking results) looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pipe = Pipeline([('gen_cl', general_cleaner()),\n",
    "                       ('processing', processing_pipe),\n",
    "                       ('scl', df_scaler()), \n",
    "                       ('lasso', Lasso(alpha=0.01))])\n",
    "\n",
    "res, bp = grid_search(train_set, y, lasso_pipe, \n",
    "            param_grid={'processing__num_pipe__transf__SF_room': [True, False],  # here it is important to specify the values to try for the grid search algorithm\n",
    "                        'processing__num_pipe__imputer__strategy': ['mean', 'median'],\n",
    "                        'processing__cat_pipe__dummies__drop_first': [True, False],\n",
    "                        'lasso__alpha': [0.1, 0.01, 0.001]},\n",
    "            cv=folds, scoring='neg_mean_squared_error')\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the best parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note how we refer to a specific parameter by calling every step of the pipeline by its name and concatenating those names by the double underscore.\n",
    "\n",
    "We see from the GridSearch that, if we ignore the fact that these models are vey simple and not high-performing, the best configurations of parameters are scoring very similar results. One may want to be sure that the model is really the best possible one and/or it is predicting reasonable prices.\n",
    "\n",
    "Thanks to all that effort in preserving the feature names and making sure that everything happens inside of the pipeline, this model will fit pretty much in any validation approach you might want to adopt.\n",
    "\n",
    "For example, I might be interested in seeing how the model performs in a 5-fold cross-validation setting, I might want to see how much the predictions are off, if I am missing something in the data, what are the most important features. With a few helper functions, we are going to do all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "def cv_score(df_train, y_train, kfolds, pipeline):\n",
    "    oof = np.zeros(len(df_train))\n",
    "    train = df_train.copy()\n",
    "    \n",
    "    for train_index, test_index in kfolds.split(train.values):\n",
    "            \n",
    "        trn_data = train.iloc[train_index][:]\n",
    "        val_data = train.iloc[test_index][:]\n",
    "        \n",
    "        trn_target = y_train.iloc[train_index].values.ravel()\n",
    "        val_target = y_train.iloc[test_index].values.ravel()\n",
    "        \n",
    "        pipeline.fit(trn_data, trn_target)\n",
    "\n",
    "        oof[test_index] = pipeline.predict(val_data).ravel()\n",
    "            \n",
    "    return oof\n",
    "\n",
    "\n",
    "def get_coef(pipe):\n",
    "    imp = pipe.steps[-1][1].coef_.tolist()\n",
    "    feats = pipe.steps[-2][1].get_feature_names()  # again, this is why we implemented that method\n",
    "    result = pd.DataFrame({'feat':feats,'score':imp})\n",
    "    result = result.sort_values(by=['score'],ascending=False)\n",
    "    return result\n",
    "\n",
    "def _plot_diagonal(ax):\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    low = min(xmin, xmax)\n",
    "    high = max(xmin, xmax)\n",
    "    scl = (high - low) / 100\n",
    "    \n",
    "    line = pd.DataFrame({'x': np.arange(low, high ,scl), # small hack for a diagonal line\n",
    "                         'y': np.arange(low, high ,scl)})\n",
    "    ax.plot(line.x, line.y, color='black', linestyle='--')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_predictions(data, true_label, pred_label, feature=None, hue=None, legend=False):\n",
    "    \n",
    "    tmp = data.copy()\n",
    "    tmp['Prediction'] = pred_label\n",
    "    tmp['True Label'] = true_label\n",
    "    tmp['Residual'] = tmp['True Label'] - tmp['Prediction']\n",
    "    \n",
    "    diag = False\n",
    "    alpha = 0.7\n",
    "    label = ''\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
    "    \n",
    "    if feature is None:\n",
    "        feature = 'True Label'\n",
    "        diag = True\n",
    "    else:\n",
    "        legend = 'full'\n",
    "        sns.scatterplot(x=feature, y='True Label', data=tmp, ax=ax[0], label='True',\n",
    "                         hue=hue, legend=legend, alpha=alpha)\n",
    "        label = 'Predicted'\n",
    "        alpha = 0.4\n",
    "\n",
    "    sns.scatterplot(x=feature, y='Prediction', data=tmp, ax=ax[0], label=label,\n",
    "                         hue=hue, legend=legend, alpha=alpha)\n",
    "    if diag:\n",
    "        ax[0] = _plot_diagonal(ax[0])\n",
    "    \n",
    "    sns.scatterplot(x=feature, y='Residual', data=tmp, ax=ax[1], \n",
    "                    hue=hue, legend=legend, alpha=0.7)\n",
    "    ax[1].axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    ax[0].set_title(f'{feature} vs Predictions')\n",
    "    ax[1].set_title(f'{feature} vs Residuals')\n",
    "\n",
    "lasso_oof = cv_score(train_set, y, folds, lasso_pipe)\n",
    "\n",
    "lasso_oof[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our predictions and we can see the coefficients of our regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coef(lasso_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the coefficients we can appreciate that GrLivArea and OverallQual have an higher impact on the SalePrice than the other features.\n",
    "\n",
    "Now we want to see if the predictions are too far off or if there is something odd in the residual plot (I suggest to read about them as they are very useful tools for diagnosing something wrong in your model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_set, y, lasso_oof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big outlier in our prediction and a visible pattern in the residual plot, both things that would require further investigation.\n",
    "\n",
    "We can also plot the residuals against the most important features, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_set, y, lasso_oof, feature='GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we find that our prediction so much off with respect to the real value was indeed a house too cheap for its size (to be fair, this is one of the outliers that everybody know about and they are documented in the official documentation).\n",
    "\n",
    "So far, we have used the default parameters of our pipeline but we know that there is a better configuration thanks to our GridSearch. Let's see if something changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_pipe = Pipeline([('fs', feat_sel('numeric')),\n",
    "                         ('imputer', df_imputer(strategy='mean')),  # tuned above\n",
    "                         ('transf', tr_numeric(SF_room=True))])  # tuned above\n",
    "\n",
    "\n",
    "cat_pipe = Pipeline([('fs', feat_sel('category')),\n",
    "                     ('imputer', df_imputer(strategy='most_frequent')), \n",
    "                     ('ord', make_ordinal(['BsmtQual', 'KitchenQual','GarageQual',\n",
    "                                           'GarageCond', 'ExterQual', 'HeatingQC'])), \n",
    "                     ('recode', recode_cat()), \n",
    "                     ('dummies', dummify(drop_first=True))])  # tuned above\n",
    "\n",
    "\n",
    "processing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', cat_pipe),\n",
    "                                                    ('num_pipe', numeric_pipe)])\n",
    "\n",
    "lasso_pipe = Pipeline([('gen_cl', general_cleaner()), \n",
    "                 ('processing', processing_pipe),\n",
    "                  ('scl', df_scaler()), ('lasso', Lasso(alpha=0.01))])  # tuned above\n",
    "\n",
    "lasso_oof = cv_score(train_set, y, folds, lasso_pipe)\n",
    "\n",
    "get_coef(lasso_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_set, y, lasso_oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_set, y, lasso_oof, feature='GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are a bit different, but we did not solved much. This was expected since we were not changing too much from the default.\n",
    "\n",
    "We can make further use of the fact that we are working with a pipeline and directly apply it to the test set and see if the behavior changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pred = lasso_pipe.predict(test_set)\n",
    "\n",
    "plot_predictions(test_set, y_test, lasso_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(test_set, y_test, lasso_pred, feature='GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fans of the numeric metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Score in 5-fold cv')\n",
    "print(f'\\tRMSE: {round(np.sqrt(mean_squared_error(y, lasso_oof)), 5)}')\n",
    "print(f'\\tMAE: {round(mean_absolute_error(np.expm1(y), np.expm1(lasso_oof)), 2)} dollars')\n",
    "print('Score on holdout test')\n",
    "print(f'\\tRMSE: {round(np.sqrt(mean_squared_error(y_test, lasso_pred)), 5)}')\n",
    "print(f'\\tMAE: {round(mean_absolute_error(np.expm1(y_test), np.expm1(lasso_pred)), 2)} dollars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = df_test[['Id']].copy()\n",
    "\n",
    "predictions = lasso_pipe.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we again have to put no effort to make our pipeline work on new data. This is important for 2 reasons:\n",
    "\n",
    "we can put all our effort in making the model better rather than fighting with messy code\n",
    "we are virtually ready to send our model to our client and it is ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
